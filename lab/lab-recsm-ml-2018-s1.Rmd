---
title: "Ridge Regression and the Lasso"
author: "Reto WÃ¼est"
date: "3/5/2018"
output:
  html_document:
    highlight: textmate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data

We will use data from the $2009$ British Social Attitudes Survey. You can download the data [here](bsas_short.RData).

Our response variable is:

- **imm_brit**: Out of every $100$ people in Britain, how many do you think are immigrants from non-western countries?

We will use the following predictors:

- **resp_female**: Is the respondent female?
- **resp_age**: Age of the respondent
- **resp_household_size**: How many people live in respondent's household?
- **resp_party_cons**: Respondent is most likely to support the Conservative Party
- **resp_party_lab**: Respondent is most likely to support the Labour Party
- **resp_party_libdem**: Respondent is most likely to support the Liberal Democratic Party
- **resp_party_snp**: Respondent is most likely to support the Scottish National Party
- **resp_party_green**: Respondent is most likely to support the Green Party
- **resp_party_ukip**: Respondent is most likely to support the UK Independence Party
- **resp_party_bnp**: Respondent is most likely to support the British National Party
- **resp_party_other**: Respondent is most likely to support another party, no party, refused to say, or did not know
- **resp_newspaper**: Respondent normally reads a daily newspaper
- **resp_internet_hrs**: Hours the respondent spends using the internet (per week)
- **resp_religious**: Respondent regards himself/herself belonging to a particular religion
- **resp_time_current_employment**: Months respondent has spent with current employer
- **resp_urban_area**: Population density
- **resp_health**: Respondent's health
- **resp_household_income**: Total income of respondent's household

Let's have a look at the distribution of the response variable (the red dashed line shows the true percentage of non-Western immigrants):

```{r, echo=FALSE, message=FALSE}
library(foreign)
library(dplyr)
library(ggplot2)

load("bsas_short.RData")

response_count <- bsas_data %>%
  dplyr::group_by(imm_brit) %>%
  dplyr::summarise(no_imm_count = n()) %>%
  data.frame()

nw_immi <- 100 * (6.973 / 65.14)  # True percentage of non-western immigrants in the UK

bar_plot <- ggplot(data = response_count,
                   aes(x = imm_brit, y = no_imm_count)) +
  geom_vline(xintercept = nw_immi, colour = "red", linetype = "dashed") +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = no_imm_count),
            position = position_dodge(width = 0.9), vjust = -0.5, size = 2) +
  scale_x_continuous(name = "Estimated percentage of non-Western immigrants") +
  scale_y_continuous(name = "Number of respondents") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 10, margin = margin(t = 5, b = 5)),
        axis.text.y = element_text(size = 10, margin = margin(l = 5, r = 5)),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12))

bar_plot
```

The plot shows that $759$ of $1049$ respondents (or $72.4\%$) overestimate the percentage of non-Western immigrants in the UK.

## Data Preparation

```{r, echo=TRUE, message=FALSE}
library(foreign)
library(dplyr)

# Load data set
load("bsas_short.RData")

# Declare factor variables
bsas_data <- bsas_data %>%
  dplyr::mutate(resp_urban_area = factor(resp_urban_area,
                                         levels = 1:4,
                                         labels = c("rural", "rather rural",
                                                    "rather urban", "urban")),
                resp_health = factor(resp_health,
                                     levels = 0:3,
                                     labels = c("bad", "fair", "fairly good", "good")))
```

We will use the `glmnet()` function from the `glmnet` package to perform ridge regression and the Lasso. Before doing so, we use the `model.matrix()` function to create a matrix of predictors. This function automatically transforms any qualitative variables into dummy variables. This is important because `glmnet()` can only take quantitative inputs.

We remove the intercept from the matrix produced by `model.matrix()` because `glmnet()` will automatically include an intercept. We also exclude the predictor **resp_party_cons**, which will serve as the baseline in our model.

```{r, echo=TRUE}
# Matrix of predictors (remove the intercept and resp_party_cons)
x <- model.matrix(imm_brit ~ . -1 -resp_party_cons, bsas_data)

# Response variable
y <- bsas_data$imm_brit
```

## Ridge Regression

The `glmnet()` function has an `alpha` argument that determines what type of model is fit. If `alpha = 0`, then a ridge regression model is fit, and if `alpha = 1`, then a Lasso model is fit. We first fit a ridge regression model to our data. Note that by default, the `glmnet()` function standardizes the variables so that they are on the same scale.

```{r, echo=TRUE, message=FALSE}
library(glmnet)

# Split data into training and test sets
train <- sample(1:nrow(x), nrow(x) / 2)
y_test <- y[-train]

# Define grid of lambda values
grid <- 10^seq(10, -2, length = 100)

# Fit ridge regression models using the glmnet function
ridge_out <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid)

# Plot shrinkage of coefficient estimates
plot(ridge_out, xvar = "lambda", label = TRUE)
```

Associated with each value of $\lambda$ is a vector of ridge regression coefficients, stored in a matrix that can be accessed by `coef()`. In our case, this is a $23 \times 100$ matrix, with a row for each predictor and the intercept and a column for each value of $\lambda$.

```{r, echo=TRUE}
dim(coef(ridge_out))
```

We now use cross-validation to choose the optimal tuning parameter $\lambda$. We can do this using the built-in cross-validation function, `cv.glmnet()`. By default, the function performs $10$-fold cross-validation, though we can change this using the argument `nfolds`. We here do $5$-fold cross-validation. We also split our data into a training data set and a test data set, so that we can estimate the test error.

```{r, echo=TRUE}
set.seed(1234)

# Perform cross-validation
ridge_cv_out <- cv.glmnet(x[train, ], y[train], alpha = 0, lambda = grid, nfolds = 5)

# Plot estimated test MSE for each lambda value
plot(ridge_cv_out)

# Determine optimal lambda value
(lambda_opt <- ridge_cv_out$lambda.min)
```

We see that the optimal value of $\lambda$ is $2.009233$. What is the test MSE associated with this optimal value?

```{r, echo=TRUE}
ridge_out_pred <- predict(ridge_out, s = lambda_opt, newx = x[-train, ])
mean((ridge_out_pred - y_test)^2)
```

Next, let's see what the benefit is of performing ridge regression instead of least squares regression.

```{r, echo=TRUE}
ridge_out_pred <- predict(ridge_out, s = 0, x = x[train, ], y = y[train],
                          newx = x[-train, ], exact = TRUE)
mean((ridge_out_pred - y_test)^2)
```

Finally, we re-fit our ridge regression model on the full data set, using the optimal value of $\lambda$.

```{r, echo=TRUE}
ridge_final <- glmnet(x, y, alpha = 0)
predict(ridge_final, type = "coefficients", s = lambda_opt)[1:23, ]
```

## The Lasso

In order to fit a Lasso model, we again use the `glmnet()` function. However, we now use the argument `alpha = 1`.

```{r, echo=TRUE}
# Fit Lasso regression models using the glmnet function
lasso_out <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)

# Plot shrinkage of coefficient estimates
plot(lasso_out, xvar = "lambda", label = TRUE)
```

We now perform cross-validation and compute the associated test error.

```{r, echo=TRUE}
set.seed(1234)

# Perform cross-validation
lasso_cv_out <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = grid, nfolds = 5)

# Plot estimated test MSE for each lambda value
plot(lasso_cv_out)

# Determine optimal lambda value
(lambda_opt <- lasso_cv_out$lambda.min)

# Fit Lasso model using the optimal lambda value
lasso_out_pred <- predict(lasso_out, s = lambda_opt, newx = x[-train, ])

# Test error
mean((lasso_out_pred - y_test)^2)
```

This value is very similar to the test MSE of ridge regression with $\lambda$ chosen by cross-validation.

Finally, we again re-fit our Lasso regression model on the full data set, using the optimal value of $\lambda$.

```{r, echo=TRUE}
lasso_final <- glmnet(x, y, alpha = 1)
predict(lasso_final, type = "coefficients", s = lambda_opt)[1:23, ]
```
